{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b83ee5df-3484-4f82-bb5d-281e1fcef684",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### CS5234J: Summative Group Assessment 3\n",
    "**Goals**: In this assignment you will be practising more advanced tools that will be useful \n",
    "in your final projects. The assignment requires you to solve three realistic data processing problems \n",
    "on the Spark platform.\n",
    "\n",
    "**Before you start:**\n",
    "* This assignment is **summative** coursework.\n",
    "* It constitutes 4% of the final course mark.\n",
    "* It consists of 3 questions.\n",
    "* The answers should be given by filling in blanks in the code cells of a copy of this \n",
    "notebook as instructed in the question descriptions and the comments in the code.\n",
    "* Do **not** create your own cells as these will not be checked!\n",
    "* Submission deadline is **21 June 2021, 10:00**\n",
    "* Submit a copy of this notebook with your answers by following the Assignment 3 \n",
    "submission link on Moodle. For example, if viewing the notebook in Jupyter, select `File->Download as->Notebook (.ipynb)` to download a copy of the notebook.\n",
    "* Please note that submitting anything rather than a copy of this notebook (e.g., a PDF file\n",
    "or a ZIP archive) will automatically result in your entire submission receiving a mark of 0. \n",
    "Likewise, any code cells that do not compile (for whatever reason, including\n",
    "accidental comments, incorrect indentation, unbalanced parentheses, etc.) will be penalized by deducting the **entire** \n",
    "quantity of marks associated with the relevant question. This is in line with the requirements \n",
    "of the departmental policy for electronic submissions: \n",
    "https://intranet.royalholloway.ac.uk/computerscience/documents/pdf/electronicsubmissionstudentversion.pdf\n",
    "* You can work in teams of **two** people. \n",
    "* If you formed a team for Assignment 1, you **must** work as part of the same team for this assignment and the final project.\n",
    "\n",
    "**Running the code**\n",
    "To run the code, we recommend using an instance of the Jupyter Notebook server integrated with \n",
    "PySpark, which can be accessed as follows:\n",
    "* Start NoMachine, and log into `linux.cim.rhul.ac.uk`\n",
    "* Open a terminal window\n",
    "* At the prompt, type `ssh -X bigdata`. Note the `X` must be capitalized.\n",
    "* Type `/home/local/ufac001/pyspark-jupyter.sh` and hit `enter` \n",
    "    to launch a Jupyter Notebook server integrated with \n",
    "PySpark. If everything works as expected, this will open up a tab in a web browser through which\n",
    "you could load and work on the notebook.\n",
    "\n",
    "As an alternative, you can also use the Databricks Community Edition cloud, but please be\n",
    "aware that their automated notebook synchronisation may not always work as expected\n",
    "potentially resulting in the loss of work. One possible workaround is to connect your notebook\n",
    "to a Git repository, and then use the provided \n",
    "commit interface to force synchronisation as necessary. If \n",
    "you would like to follow this route, and need help creating a private repository\n",
    "on GitHub (available to all RHUL students), please contact the CS Helpdesk.\n",
    "\n",
    "**Spark Restrictions**\n",
    "Your solution should use pyspark and the RDD APIs. In particular, you should *not* use\n",
    "DataFrames/DataSets or SparkSQL as part of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e0c794e-7966-4cc6-90ab-d23a499d92be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 1: Spark Term Frequencies (10%)\n",
    "Write a function `get_term_frequencies(email_lst)` that computes the _term frequency_ for each term (word) that occurs in the body of a collection of emails. The function takes one argument `email_lst`, which is a list of key-value pairs `(EMAIL-ID, BODY)` where `EMAIL-ID` is a string identifier for an email and `BODY` is a string corresponding to the text content of the email body. Your function should return a list where each element is a pair `(EMAIL-ID, [(TERM, FREQ)])`. The key `EMAIL-ID` is an email identifier and the value is itself a list of pairs containing for every term `TERM` in the email its frequency of occurrence _in that email_.\n",
    "\n",
    "Your solution may make use of the provided helper function `term_freq(body_text)`, which computes the term frequencies for a single email body, returning the result as a dictionary.\n",
    "\n",
    "Your code should be written as a series of the following Spark transformations:\n",
    "1. Use `sc.parallelize()` to create a base RDD from `email_lst`\n",
    "2. Use `mapValues` to compute using the helper function `term_freq` the term frequencies for every email body in the base RDD created in step 1. Each element of the resulting RDD should be a tuple `(EMAIL-ID, {TERM : FREQ})`, such that the term frequencies for each email are stored as a dictionary. \n",
    "3. Use `mapValues` again to convert the RDD in step 2 to another key-value pair RDD with elements `(EMAIL-ID, [(TERM, FREQ)])`, where the term frequencies for each email are stored as a list of pairs of term frequencies.\n",
    "4. Apply a `collect()` action to the result of step 3, and return the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18c7768a-b8d0-4c27-829f-820dc5111c5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "def dict_inc(dic, k):\n",
    "    dic[k] = dic.get(k, 0) + 1\n",
    "    return dic\n",
    "\n",
    "def term_freq(text):\n",
    "    r = re.compile('^[a-zA-Z]+$')\n",
    "    terms = filter(lambda x: re.match(r, x), text.split())\n",
    "\n",
    "    tf = reduce(lambda tfs, x: dict_inc(tfs, x), terms, {})\n",
    "    n_words = len(tf.keys())\n",
    "\n",
    "    # normalize to length of document\n",
    "    tf_norm = {term :  f / n_words for (term, f) in tf.items()}\n",
    "\n",
    "    return tf_norm\n",
    "\n",
    "def get_term_frequencies(email_lst):\n",
    "    '''\n",
    "    email_lst: A list of email bodies (EMAIL-ID, BODY)\n",
    "    Returns an RDD where each element (EMAIL-ID, [(TERM, FREQ)]) contains the term frequency\n",
    "    FREQ (a float) for every term TERM that occurs in the email, computed\n",
    "    using a series of Spark operations as described in the question.\n",
    "\n",
    "    Replace pass with your code. Use `sc` to reference the Spark context.\n",
    "    '''\n",
    "    # Your code goes here\n",
    "    data = sc.parallelize(email_lst)\n",
    "    return data.mapValues(lambda x: term_freq(x))\\\n",
    "          .mapValues(lambda x: [(key, value) for key, value in x.items()])\\\n",
    "          .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26b99d90-7bcc-4bd2-8277-8c54fda6fadd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can use the following code to test your implementation of `get_term_frequencies()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ce9eacd-4421-43a1-bd6d-808b6154a2b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emails/1.txt', [('Attached', 0.05), ('is', 0.1), ('the', 0.15), ('Please', 0.1), ('be', 0.05), ('advised', 0.05), ('that', 0.05), ('it', 0.05), ('password', 0.05), ('review', 0.05), ('last', 0.05), ('Hotsheet', 0.05), ('email', 0.1), ('or', 0.05), ('reply', 0.05), ('to', 0.05), ('this', 0.05), ('if', 0.05), ('you', 0.05), ('forget', 0.05)])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe output produced by the line above when executed with the model implementation\\nof get_term_frequencies() was as follows (N.B. we only print the first element of the\\nresulting list):\\n\\n[('emails/1.txt', \\n    [('Attached', 0.05), ('is', 0.1), ('the', 0.15), ('Please', 0.1), ('be', 0.05), ('advised', 0.05), \\n    ('that', 0.05), ('it', 0.05), ('password', 0.05), ('review', 0.05), ('last', 0.05), ('Hotsheet', 0.05), ('email', 0.1), \\n    ('or', 0.05), ('reply', 0.05), ('to', 0.05), ('this', 0.05), ('if', 0.05), ('you', 0.05), ('forget', 0.05)])]\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email1 = (\"emails/1.txt\", \"\"\"Attached is the Hotsheet.   \\nPlease be advised that it is password protected.\n",
    "                Please review the last Hotsheet email or reply to this email if you forget\n",
    "                the password.\"\"\")\n",
    "\n",
    "email2 = (\"emails/2.txt\", \"\"\" As you discussed with Jenn Staton, we will be\\nreviewing the daily DPR's\n",
    "                tomorrow morning to determine the May curve shift\\nreports that we would like to obtain.\n",
    "                Also, since earnings release has\\nbeen tentatively scheduled for July 12th, we would like\n",
    "                to review the June\\ncurve shift reports to date and make a portion of our June request\n",
    "                now in\\norder to alleviate some of the burden on your group in July.  Let me know\\nif\n",
    "                this poses any problems.\"\"\")\n",
    "\n",
    "email3 = (\"emails/3.txt\", \"\"\"Let\\'s all conference sometime Monday to sort \\nthrough how these trades should be documented,\n",
    "                booked and otherwise handled.  \\nLynn has suggested that ESA is merely a holding company and\n",
    "                therefore, an \\ninappropriate vehicle for holding trades (maybe one or two trades is O.K.).\"\"\")\n",
    "    \n",
    "print(get_term_frequencies([email1, email2, email3])[:1])\n",
    "'''\n",
    "The output produced by the line above when executed with the model implementation\n",
    "of get_term_frequencies() was as follows (N.B. we only print the first element of the\n",
    "resulting list):\n",
    "\n",
    "[('emails/1.txt', \n",
    "    [('Attached', 0.05), ('is', 0.1), ('the', 0.15), ('Please', 0.1), ('be', 0.05), ('advised', 0.05), \n",
    "    ('that', 0.05), ('it', 0.05), ('password', 0.05), ('review', 0.05), ('last', 0.05), ('Hotsheet', 0.05), ('email', 0.1), \n",
    "    ('or', 0.05), ('reply', 0.05), ('to', 0.05), ('this', 0.05), ('if', 0.05), ('you', 0.05), ('forget', 0.05)])]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "599d7e2c-9f68-4977-85a1-f0cce2b86248",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 2: Spark Inverse Document Frequencies (50%)\n",
    "\n",
    "Write another function `get_inv_document_frequencies(term_freqs)` that applies further transformations to the dataset returned by the `get_term_frequencies(email_lst)` function from Question 1 to determine the _inverse document frequency_ for each term that occurs in the email dataset. The inverse document frequency is a measure that allows to identify term that occur infrequently in the dataset.\n",
    "\n",
    "Your code should be written as a series of the following Spark transformations:\n",
    "\n",
    "1. Create from `tfRDD` below a new RDD whose elements are of the form `(EMAIL-ID, TERM)`, such that every element `(EMAIL-ID, [(TERM, FREQ)])` of `tfRDD` produces a separate pair in the new RDD for every term in the email. _Hint:_ Use `flatMapValues()`.\n",
    "2. Using a series of Spark transformations, convert the RDD produced in step 1 to a new RDD of document frequencies. Each element should be a tuple `(TERM, DOC-FREQ)` where `DOC-FREQ` is a _count_ of the number of emails containing the term.\n",
    "3. Use a `map` transformation to convert each `(TERM, DOC-FREQ)` tuple to a tuple `(TERM, INV-DOC-FREQ)`, where `INV-DOC-FREQ` is the _inverse document frequency_. You may use the helper function `inv_doc_freq` below to compute the inverse document frequency for each term. \n",
    "4. Apply a `collect()` action to the result of step 3, and return the resulting list.\n",
    "\n",
    "Note that similarly to Assignment 2, calling `get_term_frequencies()` first is only needed to prevent any errors in the implementation of Question 1 from propagating to the solution of this question as this way, we will be able to use the model implementation of `get_term_frequencies()` for testing. A more efficient solution would avoid materializing the results of `get_term_frequencies()` in the driver, and instead directly extend the processing steps of Question 1 with further operations. Make sure you understand why it is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57bfaad7-5324-4c31-a5ee-c1b2d39950b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def inv_doc_freq(doc_freq, n):\n",
    "    '''\n",
    "    Helper function that returns the inverse document frequency\n",
    "    given the term's document frequency and the total\n",
    "    number of documents.\n",
    "    doc_freq: the document frequency of a particular term\n",
    "    n: the total number of documents.\n",
    "    '''        \n",
    "    return max(0, math.log(n/(doc_freq+1)))\n",
    "    \n",
    "def get_inv_document_frequencies(term_freqs):\n",
    "    '''\n",
    "    Computes using a series of Spark transformations a list of \n",
    "    inverse document frequency pairs (TERM, INV-DOC-FREQ),\n",
    "    where TERM is a term.\n",
    "    \n",
    "    term_freqs: A list of pairs (EMAIL-ID, [(TERM, FREQ)]) of \n",
    "    email identifiers together with their term frequencies.\n",
    "    '''\n",
    "    # Your code below\n",
    "    tfRDD = sc.parallelize(term_freqs)\n",
    "    doc_count = tfRDD.map(lambda x: x[0]).count()\n",
    "    term_rdd = tfRDD.flatMapValues(lambda term_freqs: [term for term, freq in term_freqs])\n",
    "    term_doc_freq = term_rdd.map(lambda x: (x[1], x[0])).groupByKey().mapValues(lambda x: len(x))\n",
    "    inverse_doc_freq = term_doc_freq.mapValues(lambda x: inv_doc_freq(x, doc_count))\n",
    "    return inverse_doc_freq.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1bba6054-dd3c-4c01-9238-df0e4fcca904",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You may use the following code to test your implementation of `get_inv_document_frequencies()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "88e315d7-ba06-44da-9393-45a785a0f20e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jenn', 0.4054651081081644), ('we', 0.4054651081081644), ('tomorrow', 0.4054651081081644), ('May', 0.4054651081081644), ('curve', 0.4054651081081644), ('would', 0.4054651081081644), ('like', 0.4054651081081644), ('earnings', 0.4054651081081644), ('tentatively', 0.4054651081081644), ('July', 0.4054651081081644), ('make', 0.4054651081081644), ('of', 0.4054651081081644), ('now', 0.4054651081081644), ('in', 0.4054651081081644), ('group', 0.4054651081081644), ('Let', 0.4054651081081644), ('know', 0.4054651081081644), ('this', 0), ('sometime', 0.4054651081081644), ('Monday', 0.4054651081081644), ('these', 0.4054651081081644), ('trades', 0.4054651081081644), ('booked', 0.4054651081081644), ('suggested', 0.4054651081081644), ('ESA', 0.4054651081081644), ('is', 0), ('holding', 0.4054651081081644), ('an', 0.4054651081081644), ('two', 0.4054651081081644), ('Please', 0.4054651081081644), ('password', 0.4054651081081644), ('last', 0.4054651081081644), ('Attached', 0.4054651081081644), ('the', 0), ('be', 0), ('advised', 0.4054651081081644), ('that', 0), ('it', 0.4054651081081644), ('review', 0), ('Hotsheet', 0.4054651081081644), ('email', 0.4054651081081644), ('or', 0), ('reply', 0.4054651081081644), ('to', 0), ('if', 0), ('you', 0), ('forget', 0.4054651081081644), ('As', 0.4054651081081644), ('discussed', 0.4054651081081644), ('with', 0.4054651081081644), ('will', 0.4054651081081644), ('reviewing', 0.4054651081081644), ('daily', 0.4054651081081644), ('morning', 0.4054651081081644), ('determine', 0.4054651081081644), ('shift', 0.4054651081081644), ('reports', 0.4054651081081644), ('since', 0.4054651081081644), ('release', 0.4054651081081644), ('has', 0), ('been', 0.4054651081081644), ('scheduled', 0.4054651081081644), ('for', 0), ('June', 0.4054651081081644), ('date', 0.4054651081081644), ('and', 0), ('a', 0), ('portion', 0.4054651081081644), ('our', 0.4054651081081644), ('request', 0.4054651081081644), ('order', 0.4054651081081644), ('alleviate', 0.4054651081081644), ('some', 0.4054651081081644), ('burden', 0.4054651081081644), ('on', 0.4054651081081644), ('your', 0.4054651081081644), ('me', 0.4054651081081644), ('poses', 0.4054651081081644), ('any', 0.4054651081081644), ('all', 0.4054651081081644), ('conference', 0.4054651081081644), ('sort', 0.4054651081081644), ('through', 0.4054651081081644), ('how', 0.4054651081081644), ('should', 0.4054651081081644), ('otherwise', 0.4054651081081644), ('Lynn', 0.4054651081081644), ('merely', 0.4054651081081644), ('company', 0.4054651081081644), ('inappropriate', 0.4054651081081644), ('vehicle', 0.4054651081081644), ('one', 0.4054651081081644)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe output produced by the line above when executed with the model implementation\\nof get_inv_document_frequencies() was as follows:\\n\\n[('is', 0), ('Please', 0.4054651081081644), ('password', 0.4054651081081644), ('last', 0.4054651081081644), ('\\nthis', 0), ('Jenn', 0.4054651081081644), ('we', 0.4054651081081644), ('tomorrow', 0.4054651081081644), ('May',\\n 0.4054651081081644), ('curve', 0.4054651081081644), ('would', 0.4054651081081644), ('like', 0.405465108108164\\n4), ('earnings', 0.4054651081081644), ('tentatively', 0.4054651081081644), ('July', 0.4054651081081644), ('mak\\ne', 0.4054651081081644), ('of', 0.4054651081081644), ('now', 0.4054651081081644), ('in', 0.4054651081081644),\\n('group', 0.4054651081081644), ('Let', 0.4054651081081644), ('know', 0.4054651081081644), ('sometime', 0.40546\\n51081081644), ('Monday', 0.4054651081081644), ('these', 0.4054651081081644), ('trades', 0.4054651081081644), (\\n'booked', 0.4054651081081644), ('suggested', 0.4054651081081644), ('ESA', 0.4054651081081644), ('holding', 0.4\\n054651081081644), ('an', 0.4054651081081644), ('two', 0.4054651081081644), ('As', 0.4054651081081644), ('you',\\n 0), ('discussed', 0.4054651081081644), ('with', 0.4054651081081644), ('will', 0.4054651081081644), ('be', 0),\\n ('reviewing', 0.4054651081081644), ('the', 0), ('daily', 0.4054651081081644), ('morning', 0.4054651081081644)\\n, ('to', 0), ('determine', 0.4054651081081644), ('shift', 0.4054651081081644), ('reports', 0.4054651081081644)\\n, ('that', 0), ('since', 0.4054651081081644), ('release', 0.4054651081081644), ('has', 0), ('been', 0.40546510\\n81081644), ('scheduled', 0.4054651081081644), ('for', 0), ('review', 0), ('June', 0.4054651081081644), ('date'\\n, 0.4054651081081644), ('and', 0), ('a', 0), ('portion', 0.4054651081081644), ('our', 0.4054651081081644), ('r\\nequest', 0.4054651081081644), ('order', 0.4054651081081644), ('alleviate', 0.4054651081081644), ('some', 0.405\\n4651081081644), ('burden', 0.4054651081081644), ('on', 0.4054651081081644), ('your', 0.4054651081081644), ('me\\n', 0.4054651081081644), ('if', 0), ('poses', 0.4054651081081644), ('any', 0.4054651081081644), ('all', 0.40546\\n51081081644), ('conference', 0.4054651081081644), ('sort', 0.4054651081081644), ('through', 0.4054651081081644\\n), ('how', 0.4054651081081644), ('should', 0.4054651081081644), ('otherwise', 0.4054651081081644), ('Lynn', 0.\\n4054651081081644), ('merely', 0.4054651081081644), ('company', 0.4054651081081644), ('inappropriate', 0.405465\\n1081081644), ('vehicle', 0.4054651081081644), ('one', 0.4054651081081644), ('or', 0), ('Attached', 0.405465108\\n1081644), ('advised', 0.4054651081081644), ('it', 0.4054651081081644), ('Hotsheet', 0.4054651081081644), ('ema\\nil', 0.4054651081081644), ('reply', 0.4054651081081644), ('forget', 0.4054651081081644)]\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freqs = [('emails/1.txt', \n",
    "    [('Attached', 0.05), ('is', 0.1), ('the', 0.15), ('Please', 0.1), ('be', 0.05), ('advised', 0.05), \n",
    "    ('that', 0.05), ('it', 0.05), ('password', 0.05), ('review', 0.05), ('last', 0.05), ('Hotsheet', 0.05), ('email', 0.1), \n",
    "    ('or', 0.05), ('reply', 0.05), ('to', 0.05), ('this', 0.05), ('if', 0.05), ('you', 0.05), ('forget', 0.05)]), \n",
    "('emails/2.txt', \n",
    "    [('As', 0.017543859649122806), ('you', 0.017543859649122806), ('discussed', 0.017543859649122806), \n",
    "    ('with', 0.017543859649122806), ('Jenn', 0.017543859649122806), ('we', 0.05263157894736842), \n",
    "    ('will', 0.017543859649122806), ('be', 0.017543859649122806), ('reviewing', 0.017543859649122806), \n",
    "    ('the', 0.07017543859649122), ('daily', 0.017543859649122806), ('tomorrow', 0.017543859649122806), \n",
    "    ('morning', 0.017543859649122806), ('to', 0.08771929824561403), ('determine', 0.017543859649122806), \n",
    "    ('May', 0.017543859649122806), ('curve', 0.03508771929824561), ('shift', 0.03508771929824561), \n",
    "    ('reports', 0.03508771929824561), ('that', 0.017543859649122806), ('would', 0.03508771929824561), \n",
    "    ('like', 0.03508771929824561), ('since', 0.017543859649122806), ('earnings', 0.017543859649122806), \n",
    "    ('release', 0.017543859649122806), ('has', 0.017543859649122806), ('been', 0.017543859649122806), \n",
    "    ('tentatively', 0.017543859649122806), ('scheduled', 0.017543859649122806), ('for', 0.017543859649122806), \n",
    "    ('July', 0.017543859649122806), ('review', 0.017543859649122806), ('June', 0.03508771929824561), \n",
    "    ('date', 0.017543859649122806), ('and', 0.017543859649122806), ('make', 0.017543859649122806), \n",
    "    ('a', 0.017543859649122806), ('portion', 0.017543859649122806), ('of', 0.03508771929824561), \n",
    "    ('our', 0.017543859649122806), ('request', 0.017543859649122806), ('now', 0.017543859649122806), \n",
    "    ('in', 0.03508771929824561), ('order', 0.017543859649122806), ('alleviate', 0.017543859649122806), \n",
    "    ('some', 0.017543859649122806), ('burden', 0.017543859649122806), ('on', 0.017543859649122806), \n",
    "    ('your', 0.017543859649122806), ('group', 0.017543859649122806), ('Let', 0.017543859649122806), \n",
    "    ('me', 0.017543859649122806), ('know', 0.017543859649122806), ('if', 0.017543859649122806), \n",
    "    ('this', 0.017543859649122806), ('poses', 0.017543859649122806), ('any', 0.017543859649122806)]), \n",
    "('emails/3.txt', \n",
    "    [('all', 0.03125), ('conference', 0.03125), ('sometime', 0.03125), ('Monday', 0.03125), ('to', 0.03125), \n",
    "    ('sort', 0.03125), ('through', 0.03125), ('how', 0.03125), ('these', 0.03125), ('trades', 0.09375), ('should', 0.03125), \n",
    "    ('be', 0.03125), ('booked', 0.03125), ('and', 0.0625), ('otherwise', 0.03125), ('Lynn', 0.03125), ('has', 0.03125), \n",
    "    ('suggested', 0.03125), ('that', 0.03125), ('ESA', 0.03125), ('is', 0.0625), ('merely', 0.03125), ('a', 0.03125), \n",
    "    ('holding', 0.0625), ('company', 0.03125), ('an', 0.03125), ('inappropriate', 0.03125), ('vehicle', 0.03125), \n",
    "    ('for', 0.03125), ('one', 0.03125), ('or', 0.03125), ('two', 0.03125)])]\n",
    "\n",
    "print(get_inv_document_frequencies(term_freqs))\n",
    "\n",
    "\n",
    "'''\n",
    "The output produced by the line above when executed with the model implementation\n",
    "of get_inv_document_frequencies() was as follows:\n",
    "\n",
    "[('is', 0), ('Please', 0.4054651081081644), ('password', 0.4054651081081644), ('last', 0.4054651081081644), ('\n",
    "this', 0), ('Jenn', 0.4054651081081644), ('we', 0.4054651081081644), ('tomorrow', 0.4054651081081644), ('May',\n",
    " 0.4054651081081644), ('curve', 0.4054651081081644), ('would', 0.4054651081081644), ('like', 0.405465108108164\n",
    "4), ('earnings', 0.4054651081081644), ('tentatively', 0.4054651081081644), ('July', 0.4054651081081644), ('mak\n",
    "e', 0.4054651081081644), ('of', 0.4054651081081644), ('now', 0.4054651081081644), ('in', 0.4054651081081644),\n",
    "('group', 0.4054651081081644), ('Let', 0.4054651081081644), ('know', 0.4054651081081644), ('sometime', 0.40546\n",
    "51081081644), ('Monday', 0.4054651081081644), ('these', 0.4054651081081644), ('trades', 0.4054651081081644), (\n",
    "'booked', 0.4054651081081644), ('suggested', 0.4054651081081644), ('ESA', 0.4054651081081644), ('holding', 0.4\n",
    "054651081081644), ('an', 0.4054651081081644), ('two', 0.4054651081081644), ('As', 0.4054651081081644), ('you',\n",
    " 0), ('discussed', 0.4054651081081644), ('with', 0.4054651081081644), ('will', 0.4054651081081644), ('be', 0),\n",
    " ('reviewing', 0.4054651081081644), ('the', 0), ('daily', 0.4054651081081644), ('morning', 0.4054651081081644)\n",
    ", ('to', 0), ('determine', 0.4054651081081644), ('shift', 0.4054651081081644), ('reports', 0.4054651081081644)\n",
    ", ('that', 0), ('since', 0.4054651081081644), ('release', 0.4054651081081644), ('has', 0), ('been', 0.40546510\n",
    "81081644), ('scheduled', 0.4054651081081644), ('for', 0), ('review', 0), ('June', 0.4054651081081644), ('date'\n",
    ", 0.4054651081081644), ('and', 0), ('a', 0), ('portion', 0.4054651081081644), ('our', 0.4054651081081644), ('r\n",
    "equest', 0.4054651081081644), ('order', 0.4054651081081644), ('alleviate', 0.4054651081081644), ('some', 0.405\n",
    "4651081081644), ('burden', 0.4054651081081644), ('on', 0.4054651081081644), ('your', 0.4054651081081644), ('me\n",
    "', 0.4054651081081644), ('if', 0), ('poses', 0.4054651081081644), ('any', 0.4054651081081644), ('all', 0.40546\n",
    "51081081644), ('conference', 0.4054651081081644), ('sort', 0.4054651081081644), ('through', 0.4054651081081644\n",
    "), ('how', 0.4054651081081644), ('should', 0.4054651081081644), ('otherwise', 0.4054651081081644), ('Lynn', 0.\n",
    "4054651081081644), ('merely', 0.4054651081081644), ('company', 0.4054651081081644), ('inappropriate', 0.405465\n",
    "1081081644), ('vehicle', 0.4054651081081644), ('one', 0.4054651081081644), ('or', 0), ('Attached', 0.405465108\n",
    "1081644), ('advised', 0.4054651081081644), ('it', 0.4054651081081644), ('Hotsheet', 0.4054651081081644), ('ema\n",
    "il', 0.4054651081081644), ('reply', 0.4054651081081644), ('forget', 0.4054651081081644)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c34e88a0-8c0a-4728-a9fc-336b1f12486f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 3: Spark TF-IDF index (30%)\n",
    "Finally, write a function `tf_idf_index(term_freqs, inv_doc_freqs)` to compute the _term-frequency-inverse-document-frequency_ (TF-IDF) for every term in every email body (i.e. a TF-IDF index). Each TF-IDF value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the dataset that contain the word, which helps to adjust for the fact that some words appear more frequently in general (e.g. 'the', 'and', etc.). The TF-IDF index you will compute could potentially be used to find relevant emails given a keyword search over the dataset (e.g. by ranking each email based on the sum of the TF-IDF scores for each term in the keyword search in that email).\n",
    "\n",
    "Your code should be written as a series of Spark transformations. Starting from `tfsRDD` and `idfsRDD` below, create a new RDD where each element is a key-value pair of the form `(EMAIL-ID, [(TERM, TF-IDF)])`, i.e. the key is an email identifier and the value is a list of `(TERM, TF-IDF)` tuples, such that every term in the email is paired with its associated `TF-IDF` score _for that email_. Note that `TF_IDF` of a term in an email is simply the product of its term frequency in that email (`FREQ` from Question 1) and its inverse document frequency across all emails (`INV-DOC-FREQ` from Question 2). Finally, sort the resulting RDD by its key (`EMAIL-ID`) and return the result as a list.\n",
    "\n",
    "Note that as with the previous question, calling `get_term_frequencies()` and `get_inv_document_frequencies()` first is only needed to prevent any errors in the implementation of Question 1 from propagating to the solution of this question as this way, we will be able to use the model implementation of `get_term_frequencies()` and `get_inv_document_frequencies()` for testing. A more efficient solution would avoid materializing results in the driver, and instead directly extend the processing steps of Questions 1 and 2 with further operations. Make sure you understand why it is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "054fc889-3ab2-43e2-99b6-5f97b0a3d206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf_index(term_freqs, inv_doc_freqs):\n",
    "    tfsRDD = sc.parallelize(term_freqs)\n",
    "    idfsRDD = sc.parallelize(inv_doc_freqs)\n",
    "    # Your code here\n",
    "    return tfsRDD.flatMapValues(lambda x: x)\\\n",
    "          .map(lambda x: (x[1][0], (x[1][1], x[0])))\\\n",
    "          .join(idfsRDD)\\\n",
    "          .map(lambda x: (x[1][0][1], (x[0], x[1][0][0] * x[1][1])))\\\n",
    "          .groupByKey()\\\n",
    "          .mapValues(lambda x: list(x))\\\n",
    "          .sortByKey()\\\n",
    "          .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a8c44a1-cc0f-41ab-90af-008eace53939",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You may use the following code to test your implementation of `tf_idf_index()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b6da187-57af-4ac3-b3e4-c3c1cadac471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emails/1.txt', [('last', 0.02027325540540822), ('this', 0.0), ('is', 0.0), ('Please', 0.04054651081081644), ('password', 0.02027325540540822), ('Attached', 0.02027325540540822), ('the', 0.0), ('be', 0.0), ('that', 0.0), ('email', 0.04054651081081644), ('to', 0.0), ('you', 0.0), ('forget', 0.02027325540540822), ('advised', 0.02027325540540822), ('it', 0.02027325540540822), ('review', 0.0), ('Hotsheet', 0.02027325540540822), ('or', 0.0), ('reply', 0.02027325540540822), ('if', 0.0)])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe output produced by the line above when executed with the model implementation\\nof tf_idf_index() was as follows (N.B. we only print the first element of the\\nresulting list):\\n\\n[('emails/1.txt', [('last', 0.02027325540540822), ('this', 0.0), ('is', 0.0), ('Please', 0.04054651081081644), \\n    ('password', 0.02027325540540822), ('Attached', 0.02027325540540822), ('the', 0.0), ('be', 0.0), ('that', 0.0), \\n    ('email', 0.04054651081081644), ('to', 0.0), ('you', 0.0), ('forget', 0.02027325540540822), \\n    ('advised', 0.02027325540540822), ('it', 0.02027325540540822), ('review', 0.0), ('Hotsheet', 0.02027325540540822), \\n    ('or', 0.0), ('reply', 0.02027325540540822), ('if', 0.0)])]\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freqs = [('emails/1.txt', \n",
    "    [('Attached', 0.05), ('is', 0.1), ('the', 0.15), ('Please', 0.1), ('be', 0.05), ('advised', 0.05), \n",
    "    ('that', 0.05), ('it', 0.05), ('password', 0.05), ('review', 0.05), ('last', 0.05), ('Hotsheet', 0.05), ('email', 0.1), \n",
    "    ('or', 0.05), ('reply', 0.05), ('to', 0.05), ('this', 0.05), ('if', 0.05), ('you', 0.05), ('forget', 0.05)]), \n",
    "('emails/2.txt', \n",
    "    [('As', 0.017543859649122806), ('you', 0.017543859649122806), ('discussed', 0.017543859649122806), \n",
    "    ('with', 0.017543859649122806), ('Jenn', 0.017543859649122806), ('we', 0.05263157894736842), \n",
    "    ('will', 0.017543859649122806), ('be', 0.017543859649122806), ('reviewing', 0.017543859649122806), \n",
    "    ('the', 0.07017543859649122), ('daily', 0.017543859649122806), ('tomorrow', 0.017543859649122806), \n",
    "    ('morning', 0.017543859649122806), ('to', 0.08771929824561403), ('determine', 0.017543859649122806), \n",
    "    ('May', 0.017543859649122806), ('curve', 0.03508771929824561), ('shift', 0.03508771929824561), \n",
    "    ('reports', 0.03508771929824561), ('that', 0.017543859649122806), ('would', 0.03508771929824561), \n",
    "    ('like', 0.03508771929824561), ('since', 0.017543859649122806), ('earnings', 0.017543859649122806), \n",
    "    ('release', 0.017543859649122806), ('has', 0.017543859649122806), ('been', 0.017543859649122806), \n",
    "    ('tentatively', 0.017543859649122806), ('scheduled', 0.017543859649122806), ('for', 0.017543859649122806), \n",
    "    ('July', 0.017543859649122806), ('review', 0.017543859649122806), ('June', 0.03508771929824561), \n",
    "    ('date', 0.017543859649122806), ('and', 0.017543859649122806), ('make', 0.017543859649122806), \n",
    "    ('a', 0.017543859649122806), ('portion', 0.017543859649122806), ('of', 0.03508771929824561), \n",
    "    ('our', 0.017543859649122806), ('request', 0.017543859649122806), ('now', 0.017543859649122806), \n",
    "    ('in', 0.03508771929824561), ('order', 0.017543859649122806), ('alleviate', 0.017543859649122806), \n",
    "    ('some', 0.017543859649122806), ('burden', 0.017543859649122806), ('on', 0.017543859649122806), \n",
    "    ('your', 0.017543859649122806), ('group', 0.017543859649122806), ('Let', 0.017543859649122806), \n",
    "    ('me', 0.017543859649122806), ('know', 0.017543859649122806), ('if', 0.017543859649122806), \n",
    "    ('this', 0.017543859649122806), ('poses', 0.017543859649122806), ('any', 0.017543859649122806)]), \n",
    "('emails/3.txt', \n",
    "    [('all', 0.03125), ('conference', 0.03125), ('sometime', 0.03125), ('Monday', 0.03125), ('to', 0.03125), \n",
    "    ('sort', 0.03125), ('through', 0.03125), ('how', 0.03125), ('these', 0.03125), ('trades', 0.09375), ('should', 0.03125), \n",
    "    ('be', 0.03125), ('booked', 0.03125), ('and', 0.0625), ('otherwise', 0.03125), ('Lynn', 0.03125), ('has', 0.03125), \n",
    "    ('suggested', 0.03125), ('that', 0.03125), ('ESA', 0.03125), ('is', 0.0625), ('merely', 0.03125), ('a', 0.03125), \n",
    "    ('holding', 0.0625), ('company', 0.03125), ('an', 0.03125), ('inappropriate', 0.03125), ('vehicle', 0.03125), \n",
    "    ('for', 0.03125), ('one', 0.03125), ('or', 0.03125), ('two', 0.03125)])]\n",
    "\n",
    "\n",
    "\n",
    "inv_doc_freqs = [('is', 0), ('Please', 0.4054651081081644), ('password', 0.4054651081081644), ('last', 0.4054651081081644), \n",
    "                 ('this', 0), ('Jenn', 0.4054651081081644), ('we', 0.4054651081081644), ('tomorrow', 0.4054651081081644), \n",
    "                 ('May', 0.4054651081081644), ('curve', 0.4054651081081644), ('would', 0.4054651081081644), \n",
    "                 ('like', 0.4054651081081644), ('earnings', 0.4054651081081644), ('tentatively', 0.4054651081081644), \n",
    "                 ('July', 0.4054651081081644), ('make', 0.4054651081081644), ('of', 0.4054651081081644), \n",
    "                 ('now', 0.4054651081081644), ('in', 0.4054651081081644), ('group', 0.4054651081081644), \n",
    "                 ('Let', 0.4054651081081644), ('know', 0.4054651081081644), ('sometime', 0.4054651081081644), \n",
    "                 ('Monday', 0.4054651081081644), ('these', 0.4054651081081644), ('trades', 0.4054651081081644), \n",
    "                 ('booked', 0.4054651081081644), ('suggested', 0.4054651081081644), ('ESA', 0.4054651081081644), \n",
    "                 ('holding', 0.4054651081081644), ('an', 0.4054651081081644), ('two', 0.4054651081081644), \n",
    "                 ('As', 0.4054651081081644), ('you', 0), ('discussed', 0.4054651081081644), ('with', 0.4054651081081644), \n",
    "                 ('will', 0.4054651081081644), ('be', 0), ('reviewing', 0.4054651081081644), ('the', 0), \n",
    "                 ('daily', 0.4054651081081644), ('morning', 0.4054651081081644), ('to', 0), ('determine', 0.4054651081081644), \n",
    "                 ('shift', 0.4054651081081644), ('reports', 0.4054651081081644), ('that', 0), ('since', 0.4054651081081644), \n",
    "                 ('release', 0.4054651081081644), ('has', 0), ('been', 0.4054651081081644), ('scheduled', 0.4054651081081644), \n",
    "                 ('for', 0), ('review', 0), ('June', 0.4054651081081644), ('date', 0.4054651081081644), ('and', 0), \n",
    "                 ('a', 0), ('portion', 0.4054651081081644), ('our', 0.4054651081081644), ('request', 0.4054651081081644), \n",
    "                 ('order', 0.4054651081081644), ('alleviate', 0.4054651081081644), ('some', 0.4054651081081644), \n",
    "                 ('burden', 0.4054651081081644), ('on', 0.4054651081081644), ('your', 0.4054651081081644), \n",
    "                 ('me', 0.4054651081081644), ('if', 0), ('poses', 0.4054651081081644), ('any', 0.4054651081081644), \n",
    "                 ('all', 0.4054651081081644), ('conference', 0.4054651081081644), ('sort', 0.4054651081081644), \n",
    "                 ('through', 0.4054651081081644), ('how', 0.4054651081081644), ('should', 0.4054651081081644), \n",
    "                 ('otherwise', 0.4054651081081644), ('Lynn', 0.4054651081081644), ('merely', 0.4054651081081644), \n",
    "                 ('company', 0.4054651081081644), ('inappropriate', 0.4054651081081644), ('vehicle', 0.4054651081081644), \n",
    "                 ('one', 0.4054651081081644), ('or', 0), ('Attached', 0.4054651081081644), ('advised', 0.4054651081081644), \n",
    "                 ('it', 0.4054651081081644), ('Hotsheet', 0.4054651081081644), ('email', 0.4054651081081644), \n",
    "                 ('reply', 0.4054651081081644), ('forget', 0.4054651081081644)]\n",
    "\n",
    "print(tf_idf_index(term_freqs, inv_doc_freqs)[:1])\n",
    "\n",
    "'''\n",
    "The output produced by the line above when executed with the model implementation\n",
    "of tf_idf_index() was as follows (N.B. we only print the first element of the\n",
    "resulting list):\n",
    "\n",
    "[('emails/1.txt', [('last', 0.02027325540540822), ('this', 0.0), ('is', 0.0), ('Please', 0.04054651081081644), \n",
    "    ('password', 0.02027325540540822), ('Attached', 0.02027325540540822), ('the', 0.0), ('be', 0.0), ('that', 0.0), \n",
    "    ('email', 0.04054651081081644), ('to', 0.0), ('you', 0.0), ('forget', 0.02027325540540822), \n",
    "    ('advised', 0.02027325540540822), ('it', 0.02027325540540822), ('review', 0.0), ('Hotsheet', 0.02027325540540822), \n",
    "    ('or', 0.0), ('reply', 0.02027325540540822), ('if', 0.0)])]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57136153-b672-445f-ae5b-8e5225588342",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2021-CS5234J-A3-pub",
   "notebookOrigID": 610087376636642,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
